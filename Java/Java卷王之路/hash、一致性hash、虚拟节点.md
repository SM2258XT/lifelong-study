---
title: hash、一致性hash、虚拟节点
updated: 2022-11-21T18:47:33
created: 2022-10-15T08:59:07
---

hash、一致性hash、虚拟节点
<https://blog.csdn.net/mo71105731/article/details/123364539>
2022年10月15日
8:59

**普通hash：影响范围广**
公式：hash ( key ) % nodes =
对仅仅一个节点进行增删时，由于改变了nodes，导致所有其他节点的数据必须重新hash。
总结：影响的是所有节点，不仅仅是新增/删的节点

**一致性hash：为分布式而生**
希望尽可能多的命中原有节点，而不是原有节点全部不变

![](C:\Users\82609\AppData\Local\Temp\Java\pandoc/media/image1.png)![](C:\Users\82609\AppData\Local\Temp\Java\pandoc/media/image2.png)![](C:\Users\82609\AppData\Local\Temp\Java\pandoc/media/image3.png)

**原理：**
节点通常使用其节点的ip或者是具有唯一标示的数据 按照公式：**hash(ip) % 2^23**，将节点均匀固定到闭环上
节点和数据分布：
- 根据公式，求模的数2 ^ 23不变，说明所有节点**一旦ip确定，就直接在环上固定死了**，无论怎么增减，都不会影响到其他节点，因为模数为定值2^23！
- 数据同样使用hash(key)%2^23分布到环上
从数据在圆上映射的位置开始，顺时针方向找到的第一个节点即为存储key的节点（节点逆时针的最小⚪都是该节点数据）
增删节点：
只会影响当前节点 + 后继节点 之间数据的归属问题，对于其他节点没任何影响。
注意：
实际节点并不是均匀分布的，也有可能全都挤在一起，导致hash倾斜

**隐患：hash倾斜**
原因：节点分布不均匀 / 删除节点
当某个节点挂掉后（负载的数据太多、宕机、人为删除等），后继节点一并承载当前节点的所有数据，瞬间压力过大也挂掉，后面所有的节点全部连续挂掉，导致缓存雪崩。
**解决：引入虚拟节点，成为Hash环**
如果我们的节点足够多，而不仅仅只是物理服务器的那几个节点，就应该可以防止服务器节点分布不均的问题了。
以A节点为例，虚拟构造出(A0,A1,A2....AN)，只要是落在这些虚拟节点上的数据，都存入A节点。
读取时也相同，顺时针获取的是A0虚拟节点，就到A节点上获取数据，这样就能解决数据分布不均的问题。
虚拟节点读写大概流程为: 数据读写 -\> 虚拟节点 -\> 真实节点 -\> 读写
